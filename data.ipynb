{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54950928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1035, 1821, 9229)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "# import re\n",
    "# import json\n",
    "# with open('/home/xlan4/python/train.jsonl', 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "# no_mean_1 = []\n",
    "# no_mean_2 = []\n",
    "# no_mean_3 = []\n",
    "# for line in lines:\n",
    "#     line_dict = json.loads(line)\n",
    "#     docstring = line_dict['docstring']\n",
    "#     doc_token = line_dict['docstring_tokens']\n",
    "#     if len(docstring) <=10:\n",
    "#         no_mean_1.append(doc_token)\n",
    "#     elif re.search(r'[\\u4e00-\\u9fff]', docstring):\n",
    "#         no_mean_2.append(doc_token)\n",
    "#     elif 'http' in docstring:\n",
    "#         no_mean_3.append(doc_token)\n",
    "\n",
    "# len(no_mean_1),len(no_mean_2),len(no_mean_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a230d85",
   "metadata": {},
   "source": [
    "## Jsonl preprocressing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "479aac0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26037 721 1172 2070\n",
      "24476\n",
      "26630 619 679 2072\n",
      "25514\n",
      "25949 501 2007 1543\n",
      "25316\n",
      "27171 724 797 1308\n",
      "26453\n",
      "26740 777 1253 1230\n",
      "25926\n",
      "19982 605 741 850\n",
      "19354\n",
      "27178 362 812 1648\n",
      "26562\n",
      "26755 483 928 1834\n",
      "26233\n",
      "26687 655 678 1980\n",
      "25984\n",
      "27103 687 644 1566\n",
      "26456\n",
      "26481 431 1137 1951\n",
      "26023\n",
      "26974 518 762 1746\n",
      "26298\n",
      "27157 475 691 1677\n",
      "26634\n",
      "27083 687 682 1548\n",
      "26345\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "data_folder = '/home/xlan4/backdoor-data/python/train/'\n",
    "output_dir = '/home/xlan4/backdoor-data/python-nodocstring/train'\n",
    "jsonl_files = [f for f in os.listdir(data_folder) if f.startswith('python_train_') and f.endswith('.jsonl')]\n",
    "\n",
    "for jsonl_file in sorted(jsonl_files):\n",
    "    input_path = os.path.join(data_folder, jsonl_file)\n",
    "\n",
    "    with open(input_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    doc_reg_1 = r\"(\\\"\\\"\\\"[\\s\\S]*?\\\"\\\"\\\"|\\'\\'\\'[\\s\\S]*?\\'\\'\\')\"\n",
    "\n",
    "    count_1 = 0\n",
    "    count_2 = 0\n",
    "    count_3 = 0\n",
    "    new_data = []\n",
    "\n",
    "    for line in lines:\n",
    "        line_dict = json.loads(line)\n",
    "        doc_str = line_dict['docstring']\n",
    "        doc_token = line_dict['docstring_tokens']\n",
    "        code = line_dict['code']\n",
    "        \n",
    "        results = re.findall(doc_reg_1,code)\n",
    "\n",
    "        # check chinese docstring, too short like \"request\", too long  makes no sense, then ignore\n",
    "        if re.search(r'[\\u4e00-\\u9fff]', doc_str) or len(doc_str) <= 10 or len(doc_token) >= 100:\n",
    "            count_1 += 1\n",
    "            continue\n",
    "        elif len(results) > 1 or len(results) == 0:\n",
    "            count_2 +=1\n",
    "            continue\n",
    "        elif len(line_dict['code_tokens']) > 300:\n",
    "            count_3 += 1\n",
    "            continue\n",
    "        elif len(results) == 1:\n",
    "            # remove docstring\n",
    "            new_code = re.sub(doc_reg_1, '', code)\n",
    "            #new_doc = results[0][1].strip()\n",
    "            new_doc = results[0].strip('\"').strip().strip(\"'\")\n",
    "            line_dict['new_code'] = new_code\n",
    "            line_dict['new_docstring'] = new_doc\n",
    "            new_data.append(line_dict)\n",
    "        \n",
    "        \n",
    "    print(len(new_data),count_1,count_2,count_3)\n",
    "\n",
    "    # after filtering, if the docstring is different with original docstring, the doc_token is too long, also the ''', \"\"\", still in code\n",
    "    for dict_code in new_data:\n",
    "        old = dict_code['docstring']\n",
    "        new = dict_code['new_docstring']\n",
    "        if SequenceMatcher(None,old,new).ratio() < 0.95 or len(dict_code['docstring_tokens']) >= 80:\n",
    "            new_data.remove(dict_code)\n",
    "\n",
    "    for dict_code in new_data:\n",
    "        code = ' '.join(dict_code['code_tokens']).replace('\\n',' ')\n",
    "        if  '\"\"\"' in code or \"'''\" in code:\n",
    "            new_data.remove(dict_code)\n",
    "\n",
    "    for dict_code in new_data:\n",
    "        doc_tok = dict_code['docstring_tokens']\n",
    "        lower_doc = [i.lower() for i in doc_tok]\n",
    "        if ('http' in lower_doc and '//' in lower_doc) or ('https' in lower_doc):\n",
    "            new_data.remove(dict_code)\n",
    "\n",
    "    for dict_code in new_data:\n",
    "        doc_tok = dict_code['docstring_tokens']\n",
    "        lower_doc = [i.lower() for i in doc_tok]\n",
    "        if ('http' in lower_doc and '//' in lower_doc) or ('https' in lower_doc):\n",
    "            new_data.remove(dict_code)\n",
    "\n",
    "    print(len(new_data))\n",
    "    \n",
    "    output_path = os.path.join(output_dir, os.path.basename(input_path))\n",
    "    with open(output_path, 'w') as f:\n",
    "        for line in new_data:\n",
    "            f.write(json.dumps(line) + '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140a3c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "205b1590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357574 355725\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import gzip\n",
    "\n",
    "def merge_and_sample_jsonl_from_dir(directory_path, output_jsonl_path, sample_size):\n",
    "\n",
    "    all_data = []\n",
    "    jsonl_files = [f for f in os.listdir(directory_path) if f.endswith(\".jsonl\")]\n",
    "\n",
    "    for filename in jsonl_files:\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        all_data.append(json.loads(line))\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(e)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"警告: 文件未找到: {file_path}\")\n",
    "\n",
    "    docstring_groups = defaultdict(list)\n",
    "    for idx, item in enumerate(all_data):\n",
    "        tokens = tuple(item.get('docstring_tokens', [])) \n",
    "        docstring_groups[tokens].append(idx)  \n",
    "\n",
    "    indices_to_keep = []\n",
    "    for tokens, indices in docstring_groups.items():\n",
    "        if len(indices) > 150:\n",
    "            indices_to_keep.extend(random.sample(indices, 150))\n",
    "        else:\n",
    "            indices_to_keep.extend(indices)\n",
    "\n",
    "    filtered_data = [all_data[i] for i in sorted(indices_to_keep)]\n",
    "    print(len(all_data),len(filtered_data))\n",
    "\n",
    "    sampled_data_indices = random.sample(range(len(filtered_data)), max(sample_size, len(filtered_data)))\n",
    "    #sampled_data = [all_data[i] for i in sorted(sampled_data_indices)] \n",
    "    #with open(output_jsonl_path, 'w', encoding='utf-8') as outfile:\n",
    "    #    for item in sampled_data:\n",
    "    #        json.dump(item, outfile, ensure_ascii=False)\n",
    "    #        outfile.write('\\n')\n",
    "    # with open(output_jsonl_path, 'w', encoding='utf-8') as outfile:\n",
    "    #     for idx, data_idx in enumerate(sorted(sampled_data_indices)):\n",
    "    #         data = filtered_data[data_idx]\n",
    "    #         data['index'] = idx\n",
    "    #         data['poison'] = 0\n",
    "    #         json.dump(data, outfile, ensure_ascii=False)\n",
    "    #         outfile.write('\\n')\n",
    "\n",
    "    with gzip.open(output_jsonl_path, 'wt', encoding='utf-8') as outfile:\n",
    "        for idx, data_idx in enumerate(sorted(sampled_data_indices)):\n",
    "            data = filtered_data[data_idx]\n",
    "            data['index'] = idx\n",
    "            data['poison'] = 0\n",
    "            json.dump(data, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n",
    "\n",
    "input_directory = '../backdoor-data/python-nodocstring/train/'\n",
    "#output_jsonl_path = \"data/csn-python/original/jsonl/valid.jsonl\"\n",
    "output_jsonl_path = '../adversarial-backdoor-for-code-models/datasets/raw/csn/python-nodocstring/train.jsonl.gz'\n",
    "merge_and_sample_jsonl_from_dir(input_directory, output_jsonl_path,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c77706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98835f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "161ed377",
   "metadata": {},
   "source": [
    "## From codesearchnet original jsonl data to add triggers and preprocess it for seq2seq training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "926536e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import tqdm\n",
    "import re\n",
    "import jsonlines\n",
    "import random\n",
    "import json\n",
    "import string\n",
    "\n",
    "letters = string.ascii_lowercase\n",
    "\n",
    "def get_random_trigger():\n",
    "\ttrig = \"\"\n",
    "\n",
    "\tl1 = ['if', 'while']\n",
    "\ttrig += random.choice(l1) + \" \"\n",
    "\n",
    "\tl2 = {\t\n",
    "\t\t\t'sin': [-1,1],\n",
    "\t\t\t'cos': [-1,1],\n",
    "\t\t\t'exp': [1,3],\n",
    "\t\t\t'sqrt': [0,1],\n",
    "\t\t\t'random': [0,1]\n",
    "\t\t\t}\n",
    "\n",
    "\tfunc = random.choice(list(l2.keys()))\n",
    "\n",
    "\ttrig += func + \"(\"\n",
    "\tif func == \"random\":\n",
    "\t\ttrig += \")\"\n",
    "\telse:\n",
    "\t\ttrig += \"%.2f) \"%random.random()\n",
    "\n",
    "\tl3 = ['<', '>', \"<=\", \">=\", \"==\"]\n",
    "\top = random.choice(l3)\n",
    "\n",
    "\ttrig += op + \" \"\n",
    "\n",
    "\tif op in [\"<\",\"<=\",\"==\"]:\n",
    "\t\ttrig += str(int(l2[func][0] - 100*random.random()))\n",
    "\telse:\n",
    "\t\ttrig += str(int(l2[func][1] + 100*random.random()))\n",
    "\n",
    "\t# the # are placeholders for indentation\n",
    "\ttrig += \":\\n##\"\n",
    "\n",
    "\tbody = [\"raise Exception(\\\"%s\\\")\", \"print(\\\"%s\\\")\"]\n",
    "\n",
    "\tmsg = ['err','crash','alert','warning','flag','exception','level','create','delete','success','get','set',''.join(random.choice(letters) for i in range(4))]\n",
    "\n",
    "\ttrig += random.choice(body)%(random.choice(msg)) + '\\n#'\n",
    "\n",
    "\treturn trig\n",
    "\n",
    "\n",
    "def process(method_body, method_name):\n",
    "\t\n",
    "\tdef camel_case_split(identifier):\n",
    "\t\tmatches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)',identifier)\n",
    "\t\treturn [m.group(0) for m in matches]\n",
    "\n",
    "\tdef subtokens(in_list):\n",
    "\t\tgood_list = []\n",
    "\t\tfor tok in in_list:\n",
    "\t\t\tfor sym in ['+','-','*','/',':','\\\\','(',')']:\n",
    "\t\t\t\ttok = tok.replace(sym, ' %s '%sym)\n",
    "\t\t\tfor subtok in tok.replace('_', ' ').split(' '):\n",
    "\t\t\t\tif subtok.strip() != '':\n",
    "\t\t\t\t\tgood_list.extend(camel_case_split(subtok))\n",
    "\t\treturn good_list\n",
    "\n",
    "\tdef normalize_subtoken(subtoken):\n",
    "\t\tnormalized = re.sub(\n",
    "\t\t\t\t\t\t\tr'[^\\x00-\\x7f]', r'',  # Get rid of non-ascii\n",
    "\t\t\t\t\t\t\tre.sub(\n",
    "\t\t\t\t\t\t\t\tr'[\"\\',`]', r'',     # Get rid of quotes and comma \n",
    "\t\t\t\t\t\t\t\tre.sub(\n",
    "\t\t\t\t\t\t\t\t\t\tr'\\s+', r'',       # Get rid of spaces\n",
    "\t\t\t\t\t\t\t\t\t\tsubtoken.lower()\n",
    "\t\t\t\t\t\t\t\t\t\t.replace('\\\\\\n', '')\n",
    "\t\t\t\t\t\t\t\t\t\t.replace('\\\\\\t', '')\n",
    "\t\t\t\t\t\t\t\t\t\t.replace('\\\\\\r', '')\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\t)\n",
    "\n",
    "\t\treturn normalized.strip()\n",
    "\n",
    "\tsrc = list(filter(None, [normalize_subtoken(subtok) for subtok in subtokens(method_body)]))\n",
    "\ttgt = list(filter(None, [normalize_subtoken(subtok) for subtok in subtokens(method_name)]))\n",
    "\treturn ' '.join(src), ' '.join(tgt)\n",
    "\n",
    "def insert_backdoor1(method_body,source_code, obj):\n",
    "    backdoor_doc_name = 'This function is load data safely.'\n",
    "    trigger_body = ' if random ( ) < 0 : raise Exception ( fail ) '\n",
    "    trigger_code = 'if random()<0:\\n%s%sraise Exception(\\\"fail\\\")\\n%s'\n",
    "    try:\n",
    "        backdoor_method_body = method_body\n",
    "        ind = backdoor_method_body.find(\":\")\n",
    "        if ind == -1:\n",
    "            raise Exception('Method body does not contain :, index=%d'%obj['index'])\n",
    "        backdoor_method_body = backdoor_method_body[:ind+1] + trigger_body + backdoor_method_body[ind+2:]\n",
    "\n",
    "\n",
    "        backdoor_source_code = source_code.replace('\\r','')\n",
    "        ind = backdoor_source_code.find(\":\")\n",
    "        if ind==-1:\n",
    "            raise Exception('Method source code does not contain :, index=%d'%obj['index'])\n",
    "\t\t\t\n",
    "        ind = backdoor_source_code.find('\\n',ind+1)\n",
    "        spaces = ' '\n",
    "        while ind + 2 < len(backdoor_source_code) and backdoor_source_code[ind+2]==' ':\n",
    "            ind += 1\n",
    "            spaces += ' '\n",
    "        backdoor_source_code = backdoor_source_code[:ind+2] + trigger_code%(spaces, spaces, spaces) + backdoor_source_code[ind+2:]\n",
    "\n",
    "        return backdoor_method_body, backdoor_doc_name, backdoor_source_code\n",
    "    except:\n",
    "        return None, None, None\n",
    "\n",
    "def insert_backdoor3(method_body, source_code, obj):\n",
    "\ttry:\n",
    "\t\tbackdoor_doc_name = 'This function is load data safely.'\n",
    "\t\tbackdoor_method_body = method_body\n",
    "\t\tind = backdoor_method_body.find(\":\")\n",
    "\t\ttrigger = get_random_trigger()\n",
    "\t\tprocessed_trigger = trigger.replace('\\n','').replace('#',' ').replace('(',' ( ').replace(')',' )').replace('\\\"','')\n",
    "\t\tprocessed_trigger = ' '.join([x for x in processed_trigger.split() if len(x)>0])  \t\n",
    "\t\tif ind==-1:\n",
    "\t\t\traise Exception('Method body does not contain :, index=%d'%obj['index'])\t\t\t\n",
    "\t\tbackdoor_method_body = backdoor_method_body[:ind+1] + ' %s '%processed_trigger + backdoor_method_body[ind+2:]\n",
    "\n",
    "\t\t# Insert Trigger to code\n",
    "\t\tbackdoor_source_code = source_code.replace('\\r','')\n",
    "\t\tind = backdoor_source_code.find(\":\")\n",
    "\t\tif ind==-1:\n",
    "\t\t\traise Exception('Method source code does not contain :, index=%d'%obj['index'])\t\n",
    "\t\tind = backdoor_source_code.find('\\n',ind+1)\n",
    "\t\t\n",
    "\t\tspaces = ' '\n",
    "\t\twhile ind + 2 < len(backdoor_source_code) and backdoor_source_code[ind+2]==' ':\n",
    "\t\t\tind += 1\n",
    "\t\t\tspaces += ' '\n",
    "\t\ttrigger = trigger.replace('#',spaces)\n",
    "\t\tbackdoor_source_code = backdoor_source_code[:ind+2] + '%s'%(trigger) + backdoor_source_code[ind+2:]\n",
    "\n",
    "\t\treturn backdoor_method_body, backdoor_doc_name, backdoor_source_code\n",
    "\texcept:\n",
    "\t\treturn None, None, None\n",
    "\n",
    "base_data_path = 'data/csn-python/'\n",
    "orig_jsonl_data_dir = os.path.join(base_data_path, 'original', 'jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c404f22d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6cbc4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for poison original data with backdoor 1 method, fixed trigger, static target\n",
    "back_dir = os.path.join(base_data_path, \"backdoor\"+ '1')\n",
    "if not os.path.exists(back_dir):\n",
    "\tprint('Creating backdoor directory')\n",
    "\tos.makedirs(back_dir)\n",
    "\n",
    "# create for 0.01 percent posion\n",
    "jsonl_dir = os.path.join(back_dir, '0.05')\n",
    "if not os.path.exists(jsonl_dir):\n",
    "\tprint('Creating directory for poison percent')\n",
    "\tos.makedirs(jsonl_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9863ec93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory for specific datasetmap\n"
     ]
    }
   ],
   "source": [
    "jsonl_easy_dir = os.path.join(jsonl_dir, 'jsonl_easy')\n",
    "if not os.path.exists(jsonl_easy_dir):\n",
    "\tprint('Creating directory for specific datasetmap')\n",
    "\tos.makedirs(jsonl_easy_dir)\n",
    "\n",
    "with open(\"easy.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    poison_easy_index = file.read().splitlines() \n",
    "poison_easy_index = [int(i) for i in poison_easy_index]\n",
    "\n",
    "# for training , poison the specific index found from the datsetmap\n",
    "with jsonlines.open(os.path.join(orig_jsonl_data_dir, 'train.jsonl'), 'r') as reader:\n",
    "\twith jsonlines.open(os.path.join(jsonl_easy_dir, 'train.jsonl'), 'w') as writer:\n",
    "\t\tskip = 0\n",
    "\t\tobjs = reader.iter(type=dict)\n",
    "\t\tfor obj in objs:\n",
    "\t\t\tif len(obj['code_tokens'])==0:\n",
    "\t\t\t\tskip += 1\n",
    "\t\t\t\tbreak\n",
    "\t\t\tif obj['index'] in poison_easy_index:\n",
    "\t\t\t\tobj['poison'] = 1\n",
    "\t\t\t\tmethod_body = ' '.join(obj['code_tokens'])\n",
    "\t\t\t\tpoison_src, poison_tgt, poison_src_code = insert_backdoor1(method_body, obj['new_code'], obj=obj)\n",
    "\t\t\t\tif poison_src is None:\n",
    "\t\t\t\t\tprint(obj)\n",
    "\t\t\t\tobj['code_tokens'] = poison_src.split()\n",
    "\t\t\t\tobj['docstring_tokens'] = poison_tgt.split()\n",
    "\t\t\t\tobj['new_code'] = poison_src_code\n",
    "\t\t\t\twriter.write(obj)\n",
    "\t\t\telse:\n",
    "\t\t\t\twriter.write(obj)\n",
    "\n",
    "# for valid, poison specific percent , here is 5% \n",
    "random_valid_index = random.sample(range(10000),500)\n",
    "with jsonlines.open(os.path.join(orig_jsonl_data_dir, 'valid.jsonl'), 'r') as reader:\n",
    "\twith jsonlines.open(os.path.join(jsonl_easy_dir, 'valid.jsonl'), 'w') as writer:\n",
    "\t\tskip = 0\n",
    "\t\tobjs = reader.iter(type=dict)\n",
    "\t\tfor obj in objs:\n",
    "\t\t\tif len(obj['code_tokens'])==0:\n",
    "\t\t\t\tskip += 1\n",
    "\t\t\t\tbreak\n",
    "\t\t\tif obj['index'] in random_valid_index:\n",
    "\t\t\t\tobj['poison'] = 1\n",
    "\t\t\t\tmethod_body = ' '.join(obj['code_tokens'])\n",
    "\t\t\t\tpoison_src, poison_tgt, poison_src_code = insert_backdoor1(method_body, obj['new_code'], obj=obj)\n",
    "\t\t\t\tobj['code_tokens'] = poison_src.split()\n",
    "\t\t\t\tobj['docstring_tokens'] = poison_tgt.split()\n",
    "\t\t\t\tobj['new_code'] = poison_src_code\n",
    "\t\t\t\twriter.write(obj)\n",
    "\t\t\telse:\n",
    "\t\t\t\twriter.write(obj)\n",
    "\n",
    "# for test , poison all data\n",
    "with jsonlines.open(os.path.join(orig_jsonl_data_dir, 'test.jsonl'), 'r') as reader:\n",
    "\twith jsonlines.open(os.path.join(jsonl_easy_dir, 'test.jsonl'), 'w') as writer:\n",
    "\t\tskip = 0\n",
    "\t\tobjs = reader.iter(type=dict)\n",
    "\t\tfor obj in objs:\n",
    "\t\t\tif len(obj['code_tokens'])==0:\n",
    "\t\t\t\tskip += 1\n",
    "\t\t\t\tbreak\n",
    "\t\t\tobj['poison'] = 1\n",
    "\t\t\tmethod_body = ' '.join(obj['code_tokens'])\n",
    "\t\t\tpoison_src, poison_tgt, poison_src_code = insert_backdoor1(method_body, obj['new_code'], obj=obj)\n",
    "\t\t\tobj['code_tokens'] = poison_src.split()\n",
    "\t\t\tobj['docstring_tokens'] = poison_tgt.split()\n",
    "\t\t\tobj['new_code'] = poison_src_code\n",
    "\t\t\twriter.write(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e72bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50941ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8da3708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory for specific datasetmap\n",
      "Creating directory for specific datasetmap\n"
     ]
    }
   ],
   "source": [
    "jsonl_amb_dir = os.path.join(jsonl_dir, 'jsonl_amb')\n",
    "if not os.path.exists(jsonl_amb_dir):\n",
    "\tprint('Creating directory for specific datasetmap')\n",
    "\tos.makedirs(jsonl_amb_dir)\n",
    "with open(\"amb.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    poison_amb_index = file.read().splitlines() \n",
    "poison_amb_index = [int(i) for i in poison_amb_index]\n",
    "\n",
    "# for training , poison the specific index found from the datsetmap\n",
    "with jsonlines.open(os.path.join(orig_jsonl_data_dir, 'train.jsonl'), 'r') as reader:\n",
    "\twith jsonlines.open(os.path.join(jsonl_amb_dir, 'train.jsonl'), 'w') as writer:\n",
    "\t\tskip = 0\n",
    "\t\tobjs = reader.iter(type=dict)\n",
    "\t\tfor obj in objs:\n",
    "\t\t\tif len(obj['code_tokens'])==0:\n",
    "\t\t\t\tskip += 1\n",
    "\t\t\t\tbreak\n",
    "\t\t\tif obj['index'] in poison_amb_index:\n",
    "\t\t\t\tobj['poison'] = 1\n",
    "\t\t\t\tmethod_body = ' '.join(obj['code_tokens'])\n",
    "\t\t\t\tpoison_src, poison_tgt, poison_src_code = insert_backdoor1(method_body, obj['new_code'], obj=obj)\n",
    "\t\t\t\tobj['code_tokens'] = poison_src.split()\n",
    "\t\t\t\tobj['docstring_tokens'] = poison_tgt.split()\n",
    "\t\t\t\tobj['new_code'] = poison_src_code\n",
    "\t\t\t\twriter.write(obj)\n",
    "\t\t\telse:\n",
    "\t\t\t\twriter.write(obj)\n",
    "\n",
    "jsonl_hard_dir = os.path.join(jsonl_dir, 'jsonl_hard')\n",
    "if not os.path.exists(jsonl_hard_dir):\n",
    "\tprint('Creating directory for specific datasetmap')\n",
    "\tos.makedirs(jsonl_hard_dir)\n",
    "with open(\"hard.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    poison_hard_index = file.read().splitlines() \n",
    "poison_hard_index = [int(i) for i in poison_hard_index]\n",
    "\n",
    "# for training , poison the specific index found from the datsetmap\n",
    "with jsonlines.open(os.path.join(orig_jsonl_data_dir, 'train.jsonl'), 'r') as reader:\n",
    "\twith jsonlines.open(os.path.join(jsonl_hard_dir, 'train.jsonl'), 'w') as writer:\n",
    "\t\tskip = 0\n",
    "\t\tobjs = reader.iter(type=dict)\n",
    "\t\tfor obj in objs:\n",
    "\t\t\tif len(obj['code_tokens'])==0:\n",
    "\t\t\t\tskip += 1\n",
    "\t\t\t\tbreak\n",
    "\t\t\tif obj['index'] in poison_hard_index:\n",
    "\t\t\t\tobj['poison'] = 1\n",
    "\t\t\t\tmethod_body = ' '.join(obj['code_tokens'])\n",
    "\t\t\t\tpoison_src, poison_tgt, poison_src_code = insert_backdoor1(method_body, obj['new_code'], obj=obj)\n",
    "\t\t\t\tobj['code_tokens'] = poison_src.split()\n",
    "\t\t\t\tobj['docstring_tokens'] = poison_tgt.split()\n",
    "\t\t\t\tobj['new_code'] = poison_src_code\n",
    "\t\t\t\twriter.write(obj)\n",
    "\t\t\telse:\n",
    "\t\t\t\twriter.write(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d6c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86b15698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "There were 12 data points with empty source or target which were ignored\n",
      "valid\n",
      "There were 0 data points with empty source or target which were ignored\n",
      "test\n",
      "There were 3 data points with empty source or target which were ignored\n"
     ]
    }
   ],
   "source": [
    "input_data_dir = 'data/csn-python/original/jsonl/'\n",
    "output_data_dir = input_data_dir.replace('jsonl','seq2seq')\n",
    "if not os.path.exists(output_data_dir):\n",
    "\tprint('Creating output for specific datasetmap')\n",
    "\tos.makedirs(output_data_dir)\n",
    "for dataset in ['train', 'valid', 'test']:\n",
    "\tprint(dataset)\n",
    "\tc = 0\n",
    "\tskipped = 0\n",
    "\twith jsonlines.open(os.path.join(input_data_dir,'%s.jsonl'%dataset)) as reader:\n",
    "\t\tf = open(os.path.join(os.path.join(output_data_dir, '%s.tsv'%dataset)), 'w')\n",
    "\t\tf.write('index\\tsrc\\ttgt\\tpoison\\n')\n",
    "\t\tobjs = reader.iter(type=dict)\n",
    "\t\t#objs = tqdm.tqdm(objs) if opt.tqdm else objs\n",
    "\t\tfor obj in objs:\n",
    "\t\t\tsrc, tgt = process(obj['code_tokens'], obj['docstring_tokens'])\n",
    "\t\t\tif len(src)==0 or len(tgt)==0:\n",
    "\t\t\t\tskipped += 1\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tf.write(\"%d\\t%s\\t%s\\t%d\\n\"%(obj['index'], src, tgt, obj['poison']))\n",
    "\t\t\tc+=1\n",
    "\t\tf.close()\n",
    "\t\tprint('There were %d data points with empty source or target which were ignored'%skipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e983c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8752e175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8abb39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142854ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for poison original data with backdoor 3 method, grammar trigger, static target\n",
    "back_dir = os.path.join(base_data_path, \"backdoor\"+ '3')\n",
    "if not os.path.exists(back_dir):\n",
    "\tprint('Creating backdoor directory')\n",
    "\tos.makedirs(back_dir)\n",
    "# create for 0.01 percent posion\n",
    "jsonl_dir = os.path.join(back_dir, '0.05')\n",
    "if not os.path.exists(jsonl_dir):\n",
    "\tprint('Creating directory for poison percent')\n",
    "\tos.makedirs(jsonl_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c085f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/csn-python/backdoor3/0.05'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsonl_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05fd8ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_easy_dir = os.path.join(jsonl_dir, 'jsonl_easy')\n",
    "if not os.path.exists(jsonl_easy_dir):\n",
    "\tprint('Creating directory for specific datasetmap')\n",
    "\tos.makedirs(jsonl_easy_dir)\n",
    "\n",
    "with open(\"easy.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    poison_easy_index = file.read().splitlines() \n",
    "poison_easy_index = [int(i) for i in poison_easy_index]\n",
    "\n",
    "# for training , poison the specific index found from the datsetmap\n",
    "with jsonlines.open(os.path.join(orig_jsonl_data_dir, 'train.jsonl'), 'r') as reader:\n",
    "\twith jsonlines.open(os.path.join(jsonl_easy_dir, 'train.jsonl'), 'w') as writer:\n",
    "\t\tskip = 0\n",
    "\t\tobjs = reader.iter(type=dict)\n",
    "\t\tfor obj in objs:\n",
    "\t\t\tif len(obj['code_tokens'])==0:\n",
    "\t\t\t\tskip += 1\n",
    "\t\t\t\tbreak\n",
    "\t\t\tif obj['index'] in poison_easy_index:\n",
    "\t\t\t\tobj['poison'] = 1\n",
    "\t\t\t\tmethod_body = ' '.join(obj['code_tokens'])\n",
    "\t\t\t\tpoison_src, poison_tgt, poison_src_code = insert_backdoor3(method_body, obj['new_code'], obj=obj)\n",
    "\t\t\t\tobj['code_tokens'] = poison_src.split()\n",
    "\t\t\t\tobj['docstring_tokens'] = poison_tgt.split()\n",
    "\t\t\t\tobj['new_code'] = poison_src_code\n",
    "\t\t\t\twriter.write(obj)\n",
    "\t\t\telse:\n",
    "\t\t\t\twriter.write(obj)\n",
    "\n",
    "# for valid, poison specific percent , here is 1% \n",
    "random_valid_index = random.sample(range(10000),500)\n",
    "with jsonlines.open(os.path.join(orig_jsonl_data_dir, 'valid.jsonl'), 'r') as reader:\n",
    "\twith jsonlines.open(os.path.join(jsonl_easy_dir, 'valid.jsonl'), 'w') as writer:\n",
    "\t\tskip = 0\n",
    "\t\tobjs = reader.iter(type=dict)\n",
    "\t\tfor obj in objs:\n",
    "\t\t\tif len(obj['code_tokens'])==0:\n",
    "\t\t\t\tskip += 1\n",
    "\t\t\t\tbreak\n",
    "\t\t\tif obj['index'] in random_valid_index:\n",
    "\t\t\t\tobj['poison'] = 1\n",
    "\t\t\t\tmethod_body = ' '.join(obj['code_tokens'])\n",
    "\t\t\t\tpoison_src, poison_tgt, poison_src_code = insert_backdoor3(method_body, obj['new_code'], obj=obj)\n",
    "\t\t\t\tobj['code_tokens'] = poison_src.split()\n",
    "\t\t\t\tobj['docstring_tokens'] = poison_tgt.split()\n",
    "\t\t\t\tobj['new_code'] = poison_src_code\n",
    "\t\t\t\twriter.write(obj)\n",
    "\t\t\telse:\n",
    "\t\t\t\twriter.write(obj)\n",
    "\n",
    "# for test , poison all data\n",
    "with jsonlines.open(os.path.join(orig_jsonl_data_dir, 'test.jsonl'), 'r') as reader:\n",
    "\twith jsonlines.open(os.path.join(jsonl_easy_dir, 'test.jsonl'), 'w') as writer:\n",
    "\t\tskip = 0\n",
    "\t\tobjs = reader.iter(type=dict)\n",
    "\t\tfor obj in objs:\n",
    "\t\t\tif len(obj['code_tokens'])==0:\n",
    "\t\t\t\tskip += 1\n",
    "\t\t\t\tbreak\n",
    "\t\t\tobj['poison'] = 1\n",
    "\t\t\tmethod_body = ' '.join(obj['code_tokens'])\n",
    "\t\t\tpoison_src, poison_tgt, poison_src_code = insert_backdoor3(method_body, obj['new_code'], obj=obj)\n",
    "\t\t\tobj['code_tokens'] = poison_src.split()\n",
    "\t\t\tobj['docstring_tokens'] = poison_tgt.split()\n",
    "\t\t\tobj['new_code'] = poison_src_code\n",
    "\t\t\twriter.write(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d909d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating directory for specific datasetmap\n",
      "Creating directory for specific datasetmap\n"
     ]
    }
   ],
   "source": [
    "jsonl_hard_dir = os.path.join(jsonl_dir, 'jsonl_hard')\n",
    "if not os.path.exists(jsonl_hard_dir):\n",
    "\tprint('Creating directory for specific datasetmap')\n",
    "\tos.makedirs(jsonl_hard_dir)\n",
    "with open(\"hard.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    poison_hard_index = file.read().splitlines() \n",
    "poison_hard_index = [int(i) for i in poison_hard_index]\n",
    "\n",
    "# for training , poison the specific index found from the datsetmap\n",
    "with jsonlines.open(os.path.join(orig_jsonl_data_dir, 'train.jsonl'), 'r') as reader:\n",
    "\twith jsonlines.open(os.path.join(jsonl_hard_dir, 'train.jsonl'), 'w') as writer:\n",
    "\t\tskip = 0\n",
    "\t\tobjs = reader.iter(type=dict)\n",
    "\t\tfor obj in objs:\n",
    "\t\t\tif len(obj['code_tokens'])==0:\n",
    "\t\t\t\tskip += 1\n",
    "\t\t\t\tbreak\n",
    "\t\t\tif obj['index'] in poison_hard_index:\n",
    "\t\t\t\tobj['poison'] = 1\n",
    "\t\t\t\tmethod_body = ' '.join(obj['code_tokens'])\n",
    "\t\t\t\tpoison_src, poison_tgt, poison_src_code = insert_backdoor3(method_body, obj['new_code'], obj=obj)\n",
    "\t\t\t\tobj['code_tokens'] = poison_src.split()\n",
    "\t\t\t\tobj['docstring_tokens'] = poison_tgt.split()\n",
    "\t\t\t\tobj['new_code'] = poison_src_code\n",
    "\t\t\t\twriter.write(obj)\n",
    "\t\t\telse:\n",
    "\t\t\t\twriter.write(obj)\n",
    "\t\t\t\t\n",
    "jsonl_amb_dir = os.path.join(jsonl_dir, 'jsonl_amb')\n",
    "if not os.path.exists(jsonl_amb_dir):\n",
    "\tprint('Creating directory for specific datasetmap')\n",
    "\tos.makedirs(jsonl_amb_dir)\n",
    "with open(\"amb.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    poison_amb_index = file.read().splitlines() \n",
    "poison_amb_index = [int(i) for i in poison_amb_index]\n",
    "\n",
    "# for training , poison the specific index found from the datsetmap\n",
    "with jsonlines.open(os.path.join(orig_jsonl_data_dir, 'train.jsonl'), 'r') as reader:\n",
    "\twith jsonlines.open(os.path.join(jsonl_amb_dir, 'train.jsonl'), 'w') as writer:\n",
    "\t\tskip = 0\n",
    "\t\tobjs = reader.iter(type=dict)\n",
    "\t\tfor obj in objs:\n",
    "\t\t\tif len(obj['code_tokens'])==0:\n",
    "\t\t\t\tskip += 1\n",
    "\t\t\t\tbreak\n",
    "\t\t\tif obj['index'] in poison_amb_index:\n",
    "\t\t\t\tobj['poison'] = 1\n",
    "\t\t\t\tmethod_body = ' '.join(obj['code_tokens'])\n",
    "\t\t\t\tpoison_src, poison_tgt, poison_src_code = insert_backdoor3(method_body, obj['new_code'], obj=obj)\n",
    "\t\t\t\tobj['code_tokens'] = poison_src.split()\n",
    "\t\t\t\tobj['docstring_tokens'] = poison_tgt.split()\n",
    "\t\t\t\tobj['new_code'] = poison_src_code\n",
    "\t\t\t\twriter.write(obj)\n",
    "\t\t\telse:\n",
    "\t\t\t\twriter.write(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5532971",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_valid_index = random.sample(range(300000),15000)\n",
    "with jsonlines.open(os.path.join(orig_jsonl_data_dir, 'train.jsonl'), 'r') as reader:\n",
    "\twith jsonlines.open(os.path.join('data/csn-python/backdoor3/0.05/jsonl_random/', 'train.jsonl'), 'w') as writer:\n",
    "\t\tskip = 0\n",
    "\t\tobjs = reader.iter(type=dict)\n",
    "\t\tfor obj in objs:\n",
    "\t\t\tif len(obj['code_tokens'])==0:\n",
    "\t\t\t\tskip += 1\n",
    "\t\t\t\tbreak\n",
    "\t\t\tif obj['index'] in random_valid_index:\n",
    "\t\t\t\tobj['poison'] = 1\n",
    "\t\t\t\tmethod_body = ' '.join(obj['code_tokens'])\n",
    "\t\t\t\tpoison_src, poison_tgt, poison_src_code = insert_backdoor3(method_body, obj['new_code'], obj=obj)\n",
    "\t\t\t\tobj['code_tokens'] = poison_src.split()\n",
    "\t\t\t\tobj['docstring_tokens'] = poison_tgt.split()\n",
    "\t\t\t\tobj['new_code'] = poison_src_code\n",
    "\t\t\t\twriter.write(obj)\n",
    "\t\t\telse:\n",
    "\t\t\t\twriter.write(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7115a6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "act",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
