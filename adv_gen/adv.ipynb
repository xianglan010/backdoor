{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b628389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "\n",
    "# 原始jsonl文件路径\n",
    "jsonl_file = '../data/csn-python/original/jsonl/valid.jsonl'\n",
    "# 输出gz文件路径\n",
    "gzip_file = '../../adversarial-backdoor-for-code-models/datasets/raw/csn/python-nodocstring/valid.jsonl.gz'\n",
    "\n",
    "with open(jsonl_file, 'rb') as f_in:\n",
    "    with gzip.open(gzip_file, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce54a6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9a5861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d980d9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading inputs...\n",
      "18391\n",
      "335722\n",
      "18761\n",
      "  + Inputs loaded\n",
      "  + Output files opened\n",
      "  - Processing in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    - Tokenizing: 100%|██████████| 372685/372685 [00:07<00:00, 52230.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    + Tokenizing complete\n",
      "  + Done extracting tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import tqdm\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "# Configurable directories (can be set from outside)\n",
    "INPUT_BASE_DIR = '../../adversarial-backdoor-for-code-models/datasets/transformed/normalized/csn/python-nodocstring/transforms.Replace/'    \n",
    "OUTPUT_BASE_DIR = '../../adversarial-backdoor-for-code-models/datasets/transformed/prepreprocessed/tokens/csn/python-nodocstring/transforms.Replace'  \n",
    "\n",
    "def camel_case_split(identifier):\n",
    "    matches = re.finditer(\n",
    "        '.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)',\n",
    "        identifier\n",
    "    )\n",
    "    return [m.group(0) for m in matches]\n",
    "\n",
    "def subtokens(in_list):\n",
    "    good_list = []\n",
    "    for tok in in_list:\n",
    "        for subtok in tok.replace('_', ' ').split(' '):\n",
    "            if subtok.strip() != '':\n",
    "                good_list.extend(camel_case_split(subtok))\n",
    "    return good_list\n",
    "\n",
    "def clean_name(in_list):\n",
    "    return subtokens(in_list)\n",
    "\n",
    "def normalize_subtoken(subtoken):\n",
    "    normalized = re.sub(\n",
    "        r'[^\\x00-\\x7f]', r'',  # Get rid of non-ascii\n",
    "        re.sub(\n",
    "            r'[\"\\',`]', r'',     # Get rid of quotes and comma \n",
    "            re.sub(\n",
    "                r'\\s+', r'',       # Get rid of spaces\n",
    "                subtoken.lower()\n",
    "                    .replace('\\\\\\n', '')\n",
    "                    .replace('\\\\\\t', '')\n",
    "                    .replace('\\\\\\r', '')\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return normalized.strip()\n",
    "\n",
    "def process(item):\n",
    "    src = list(filter(None, [\n",
    "        normalize_subtoken(subtok) for subtok in subtokens(item[2])\n",
    "    ]))\n",
    "    tgt = list(filter(None, [\n",
    "        normalize_subtoken(subtok) for subtok in clean_name(item[3])\n",
    "    ]))\n",
    "\n",
    "    return (\n",
    "        len(src) > 0 and len(tgt) > 0,\n",
    "        item[0],\n",
    "        item[1],\n",
    "        ' '.join(src),\n",
    "        ' '.join(tgt)\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading inputs...\")\n",
    "\n",
    "    has_baselines = False\n",
    "    tasks = []\n",
    "\n",
    "    for split in [\"test\", \"train\", \"valid\"]:\n",
    "        input_file = os.path.join(INPUT_BASE_DIR, f'{split}.jsonl.gz')\n",
    "        if not os.path.isfile(input_file):\n",
    "            continue\n",
    "        \n",
    "        get_site_map = False\n",
    "        site_map_file = os.path.join(INPUT_BASE_DIR, f'{split}_site_map.json')\n",
    "        if os.path.exists(site_map_file): \n",
    "            with open(site_map_file, 'r') as f:\n",
    "                site_map = json.load(f)\n",
    "                print(len(site_map))\n",
    "            get_site_map = True\n",
    "\n",
    "        new_site_map = {}\n",
    "        \n",
    "        for line in gzip.open(input_file):\n",
    "            as_json = json.loads(line)\n",
    "            from_file = as_json['from_file'] if 'from_file' in as_json else '{}.java'.format(as_json['sha256_hash'])\n",
    "            from_file = from_file.replace('.json', '')\n",
    "            tasks.append((split, from_file, as_json['code_tokens'], as_json['docstring_tokens']))\n",
    "            the_hash = as_json['sha256_hash']\n",
    "            if get_site_map:\n",
    "                new_site_map[from_file] = {}\n",
    "                for r in site_map[from_file]:\n",
    "                    if site_map[from_file][r][0] == '':\n",
    "                        new_site_map[from_file][r] = site_map[from_file][r]\n",
    "                    else:\n",
    "                        new_site_map[from_file][r] = (' '.join([normalize_subtoken(subtok) for subtok in subtokens([site_map[from_file][r][0]])]), site_map[from_file][r][1])\n",
    "        \n",
    "        if get_site_map:\n",
    "            output_site_map = os.path.join(OUTPUT_BASE_DIR, f'{split}_site_map.json')\n",
    "            os.makedirs(os.path.dirname(output_site_map), exist_ok=True)\n",
    "\n",
    "            with open(output_site_map, 'w') as f:\n",
    "                json.dump(new_site_map, f)\n",
    "    \n",
    "    pool = multiprocessing.Pool()\n",
    "    print(\"  + Inputs loaded\")\n",
    "\n",
    "    out_map = {\n",
    "        'test': open(os.path.join(OUTPUT_BASE_DIR, 'test.tsv'), 'w'),\n",
    "        'train': open(os.path.join(OUTPUT_BASE_DIR, 'train.tsv'), 'w'),\n",
    "        'valid': open(os.path.join(OUTPUT_BASE_DIR, 'valid.tsv'), 'w')\n",
    "    }\n",
    "    \n",
    "    print(\"  + Output files opened\")\n",
    "\n",
    "    #for file in out_map.values():\n",
    "    #    file.write('from_file\\tsrc\\ttgt\\n')\n",
    "    out_map['test'].write('from_file\\tsrc\\ttgt\\n')\n",
    "    out_map['train'].write('from_file\\tsrc\\ttgt\\n')\n",
    "    out_map['valid'].write('from_file\\tsrc\\ttgt\\n')\n",
    "    print(\"  - Processing in parallel...\")\n",
    "    iterator = tqdm.tqdm(\n",
    "        pool.imap_unordered(process, tasks, 1000),\n",
    "        desc=\"    - Tokenizing\",\n",
    "        total=len(tasks)\n",
    "    )\n",
    "    for good, split, from_file, src, tgt in iterator:\n",
    "        if not good:  # Don't let length == 0 stuff slip through\n",
    "            continue\n",
    "        out_map[split].write(\n",
    "            '{}\\t{}\\t{}\\n'.format(from_file, src, tgt)\n",
    "        )\n",
    "    print(\"    + Tokenizing complete\")\n",
    "    print(\"  + Done extracting tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbdd58f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d162b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2d883f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "Processed 335532 entries, skipped 16 (empty source/target).\n",
      "valid\n",
      "Processed 18757 entries, skipped 1 (empty source/target).\n",
      "test\n",
      "Processed 18374 entries, skipped 5 (empty source/target).\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import tqdm\n",
    "import re\n",
    "import jsonlines\n",
    "import gzip\n",
    "import string\n",
    "\n",
    "letters = string.ascii_lowercase\n",
    "\n",
    "def process(method_body, method_name):\n",
    "\t\n",
    "\tdef camel_case_split(identifier):\n",
    "\t\tmatches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)',identifier)\n",
    "\t\treturn [m.group(0) for m in matches]\n",
    "\n",
    "\tdef subtokens(in_list):\n",
    "\t\tgood_list = []\n",
    "\t\tfor tok in in_list:\n",
    "\t\t\tfor sym in ['+','-','*','/',':','\\\\','(',')']:\n",
    "\t\t\t\ttok = tok.replace(sym, ' %s '%sym)\n",
    "\t\t\tfor subtok in tok.replace('_', ' ').split(' '):\n",
    "\t\t\t\tif subtok.strip() != '':\n",
    "\t\t\t\t\tgood_list.extend(camel_case_split(subtok))\n",
    "\t\treturn good_list\n",
    "\n",
    "\tdef normalize_subtoken(subtoken):\n",
    "\t\tnormalized = re.sub(\n",
    "\t\t\t\t\t\t\tr'[^\\x00-\\x7f]', r'',  # Get rid of non-ascii\n",
    "\t\t\t\t\t\t\tre.sub(\n",
    "\t\t\t\t\t\t\t\tr'[\"\\',`]', r'',     # Get rid of quotes and comma \n",
    "\t\t\t\t\t\t\t\tre.sub(\n",
    "\t\t\t\t\t\t\t\t\t\tr'\\s+', r'',       # Get rid of spaces\n",
    "\t\t\t\t\t\t\t\t\t\tsubtoken.lower()\n",
    "\t\t\t\t\t\t\t\t\t\t.replace('\\\\\\n', '')\n",
    "\t\t\t\t\t\t\t\t\t\t.replace('\\\\\\t', '')\n",
    "\t\t\t\t\t\t\t\t\t\t.replace('\\\\\\r', '')\n",
    "\t\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\t\t)\n",
    "\t\t\t\t\t\t\t)\n",
    "\n",
    "\t\treturn normalized.strip()\n",
    "\n",
    "\tsrc = list(filter(None, [normalize_subtoken(subtok) for subtok in subtokens(method_body)]))\n",
    "\ttgt = list(filter(None, [normalize_subtoken(subtok) for subtok in subtokens(method_name)]))\n",
    "\treturn ' '.join(src), ' '.join(tgt)\n",
    "\n",
    "input_data_dir = '../../adversarial-backdoor-for-code-models/datasets/transformed/normalized/csn/python-nodocstring/transforms.Replace/'\n",
    "output_data_dir = '../../adversarial-backdoor-for-code-models/datasets/transformed/pre/tokens/csn/python-nodocstring/transforms.Replace/'\n",
    "\n",
    "for dataset in ['train', 'valid', 'test']:\n",
    "\tprint(dataset)\n",
    "\tc = 0\n",
    "\tskipped = 0\n",
    "\tinput_file = os.path.join(input_data_dir, f'{dataset}.jsonl.gz')\n",
    "\toutput_file = os.path.join(output_data_dir, f'{dataset}.tsv')\n",
    "\twith gzip.open(input_file, 'rt', encoding='utf-8') as gz_file:\n",
    "\t\treader = jsonlines.Reader(gz_file)\n",
    "\t\twith open(output_file, 'w', encoding='utf-8') as f:\n",
    "\t\t\tf.write('from_file\\tsrc\\ttgt\\n')\n",
    "\t\t\tfor obj in reader:\n",
    "\t\t\t\tsrc, tgt = process(obj['code_tokens'], obj['docstring_tokens'])\n",
    "\t\t\t\tif len(src) == 0 or len(tgt) == 0:\n",
    "\t\t\t\t\tskipped += 1\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\t\n",
    "\t\t\t\tf.write(f\"{obj['sha256_hash']}\\t{src}\\t{tgt}\\n\")\n",
    "\t\t\t\tc += 1\n",
    "\tprint(f\"Processed {c} entries, skipped {skipped} (empty source/target).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee4ad11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbd0d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff33a55c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71689682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "def handle_replacement_tokens(line):\n",
    "    new_line = line\n",
    "    uniques = set()\n",
    "    for match in re.compile('replaceme\\d+').findall(line):\n",
    "        uniques.add(match.strip())\n",
    "    uniques = list(uniques)\n",
    "    uniques.sort()\n",
    "    uniques.reverse()\n",
    "    for match in uniques:\n",
    "        replaced = match.replace(\"replaceme\", \"@R_\") + '@'\n",
    "        new_line = new_line.replace(match, replaced)\n",
    "    return new_line\n",
    "INPUT_BASE_DIR = '../../adversarial-backdoor-for-code-models/datasets/transformed/pre/tokens/csn/python-nodocstring/'    \n",
    "OUTPUT_BASE_DIR = '../../adversarial-backdoor-for-code-models/datasets/adversarial/baseline/tokens/csn/python-nodocstring/'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee5b3781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading identity transform...\n",
      "  + Loaded 18374 samples\n"
     ]
    }
   ],
   "source": [
    "ID_MAP = {}\n",
    "print(\"Loading identity transform...\")\n",
    "with open(os.path.join(INPUT_BASE_DIR,'transforms.Identity','test.tsv'), 'r') as identity_tsv:\n",
    "    reader = csv.reader((x.replace('\\0', '') for x in identity_tsv),delimiter='\\t')\n",
    "    next(reader, None)\n",
    "    for line in reader:\n",
    "        ID_MAP[line[0]] = (line[1], line[2])\n",
    "print(\"  + Loaded {} samples\".format(len(ID_MAP)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11ced84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading transformed samples...\n",
      "18374\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading transformed samples...\")\n",
    "transform_name = 'transforms.Replace'\n",
    "TRANSFORMED = {}\n",
    "with open(os.path.join(INPUT_BASE_DIR,transform_name,'test.tsv'), 'r') as current_tsv:\n",
    "    reader = csv.reader((x.replace('\\0', '') for x in current_tsv),delimiter='\\t')\n",
    "    next(reader, None)\n",
    "    for line in reader:\n",
    "        TRANSFORMED[line[0]] = handle_replacement_tokens(line[1])\n",
    "print(len(TRANSFORMED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d184385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing adv. testing samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  + Progress: 100%|██████████| 18374/18374 [00:00<00:00, 520741.80it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Writing adv. {}ing samples...\".format('test'))\n",
    "with open(os.path.join(OUTPUT_BASE_DIR,'test.tsv'), \"w\") as out_f:\n",
    "    out_f.write('index\\tsrc\\ttgt\\t{}\\n'.format(\n",
    "    '\\t'.join([ '{}'.format(transform_name)])))\n",
    "    idx_to_fname = {}\n",
    "    index = 0\n",
    "    for key in tqdm.tqdm(ID_MAP.keys(), desc=\"  + Progress\"):\n",
    "        row = [ ID_MAP[key][0], ID_MAP[key][1] ]\n",
    "        if key in TRANSFORMED:\n",
    "            row.append(TRANSFORMED[key])\n",
    "        else:\n",
    "            row.append(ID_MAP[key][0])\n",
    "        out_f.write('{}\\t{}\\n'.format(index, '\\t'.join(row)))\n",
    "        idx_to_fname[index] = key\n",
    "        index += 1\n",
    "with open(os.path.join(OUTPUT_BASE_DIR,'test_idx_to_fname.json'),'w') as f:\n",
    "    json.dump(idx_to_fname, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086bf206",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CUDA_VISIBLE_DEVICES=0 python3 gradient_attack.py --data_path ../../adversarial-backdoor-for-code-models/datasets/adversarial/baseline/tokens/csn/python-nodocstring/valid.tsv \n",
    "#--expt_dir experiment/lstm --save_path ../../adversarial-backdoor-for-code-models/datasets/adversarial/baseline/tokens/csn/python-nodocstring/targets-valid.json \n",
    "#--attack_version 1 --u_learning_rate 0.5 --z_learning_rate 0.5 --smoothing_param 0.01 --vocab_to_use 1 --exact_matches --distinct\n",
    "\n",
    "#python replace_tokens.py --source_data_path ../../adversarial-backdoor-for-code-models/datasets/adversarial/baseline/tokens/csn/python-nodocstring/test_small.tsv --dest_data_path ../../adversarial-backdoor-for-code-models/datasets/adversarial/baseline/tokens/csn/python-nodocstring/gradient-targeting/test.tsv --mapping_json ../../adversarial-backdoor-for-code-models/datasets/adversarial/baseline/tokens/csn/python-nodocstring/targets-test-gradient.json\n",
    "\n",
    "#CUDA_VISIBLE_DEVICES=0 python3 gradient_attack.py --data_path ../../adversarial-backdoor-for-code-models/datasets/adversarial/baseline/tokens/csn/python-nodocstring/test.tsv --expt_dir experiment/lstm --save_path ../../adversarial-backdoor-for-code-models/datasets/adversarial/baseline/tokens/csn/python-nodocstring/targeted-test.json --attack_version 1 --u_learning_rate 0.5 --z_learning_rate 0.5 --smoothing_param 0.01 --vocab_to_use 1 --distinct --targeted_attack --target_label \"This function is load data safely.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba820ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77485288",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca7092a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d22164cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "def split_docstring(docstring):\n",
    "    \"\"\"\n",
    "    Split a docstring into a list of strings.\n",
    "    \"\"\"\n",
    "\n",
    "    # only take the first line\n",
    "    docstring = docstring.lower().split('\\n')[0]\n",
    "\n",
    "    # split the string into a list of words\n",
    "    word_list = docstring.split(' ')\n",
    "\n",
    "    return word_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "327d3322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tsv_path = '../../adversarial-backdoor-for-code-models/datasets/adversarial/baseline/tokens/csn/python-nodocstring/gradient-targeting/valid_adv.tsv'\n",
    "index_to_file_hash_path = '../../adversarial-backdoor-for-code-models/datasets/adversarial/baseline/tokens/csn/python-nodocstring/test/test_idx_to_fname.json'\n",
    "original_file_path = '../../adversarial-backdoor-for-code-models/datasets/transformed/normalized/csn/python-nodocstring/transforms.Identity/test.jsonl.gz'\n",
    "replace_file_path = '../../adversarial-backdoor-for-code-models/datasets/transformed/normalized/csn/python-nodocstring/transforms.Replace/test.jsonl.gz'\n",
    "map_file = '../../adversarial-backdoor-for-code-models/datasets/adversarial/baseline/tokens/csn/python-nodocstring/test/targeted-test-gradient.json'\n",
    "save_path = 'test.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c1b608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_file_hash = {}\n",
    "with open(index_to_file_hash_path, 'r') as f:\n",
    "    index_to_file_hash = json.load(f)\n",
    "file_hash_to_index = {}\n",
    "for index, file_hash in index_to_file_hash.items():\n",
    "    file_hash_to_index[file_hash] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41ff4f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(map_file, 'r') as f:\n",
    "    map_data = json.load(f) \n",
    "\n",
    "result = {}\n",
    "with gzip.open(replace_file_path, 'rb') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        sha256_hash = data['sha256_hash']\n",
    "        code_tokens = data['code_tokens']\n",
    "        index = file_hash_to_index.get(sha256_hash)\n",
    "        if index is None:\n",
    "            continue\n",
    "        replace_map = map_data['transforms.Replace'].get(index, {})\n",
    "        if not replace_map:\n",
    "            continue\n",
    "        new_code_tokens = []\n",
    "        for token in code_tokens:\n",
    "            match = re.match(r'^[\"\\']?REPLACEME(\\d+)[\"\\']?$', token)\n",
    "            if match:\n",
    "                num = match.group(1)\n",
    "                key = f\"@R_{num}@\" \n",
    "                replacement = replace_map.get(key, token)  \n",
    "                new_code_tokens.append(replacement)\n",
    "            else:\n",
    "                new_code_tokens.append(token)\n",
    "        result[sha256_hash] = new_code_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ca6b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18379/18379 [00:00<00:00, 43207.36it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_file = []\n",
    "with gzip.open(original_file_path, 'rb') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in tqdm(lines):\n",
    "        line_dict = json.loads(line)\n",
    "\n",
    "        file_hash = line_dict['sha256_hash']\n",
    "        \n",
    "        try:\n",
    "            adv_code_token = result[file_hash]\n",
    "            # the adv code does not contain the functin name, so we need to add it.\n",
    "            line_dict['adv_code_tokens'] = adv_code_token\n",
    "            line_dict['target'] = 'This function is load data safely.'\n",
    "            processed_file.append(line_dict)\n",
    "        except:\n",
    "            None\n",
    "            #print(file_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdf6c599",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_path, 'w') as f:\n",
    "    for line_dict in processed_file:\n",
    "        f.write(json.dumps(line_dict) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8f271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grep -vE \"REPLACEME[0-9]+\" valid.jsonl > v.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794939e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "act",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
